<!DOCTYPE HTML><html lang=zh-CN><head><meta charset=utf-8><meta name=keywords content="生成式人工智能导论-Chap.4, MUG-chen&#39;s Blog"><meta name=description content=生成式人工智能导论第四部分><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=no"><meta name=renderer content=webkit|ie-stand|ie-comp><meta name=mobile-web-app-capable content=yes><meta name=format-detection content="telephone=no"><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=black-translucent><meta name=referrer content=no-referrer-when-downgrade><title>生成式人工智能导论-Chap.4 | MUG-chen&#39;s Blog</title><link rel=icon type=image/png href=/favicon.png><style>body{background-image:url(https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Vivid%20theory.webp);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel=stylesheet type=text/css href=/libs/awesome/css/all.min.css><link rel=stylesheet type=text/css href=/libs/materialize/materialize.min.css><link rel=stylesheet type=text/css href=/libs/aos/aos.css><link rel=stylesheet type=text/css href=/libs/animate/animate.min.css><link rel=stylesheet type=text/css href=/libs/lightGallery/css/lightgallery.min.css><link rel=stylesheet type=text/css href=/css/matery.css><link rel=stylesheet type=text/css href=/css/my.css><link rel=stylesheet type=text/css href=/css/dark.css media=none onload='"all"!=media&&(media="all")'><link rel=stylesheet href=/libs/tocbot/tocbot.css><link rel=stylesheet href=/css/post.css><script src=/libs/jquery/jquery-3.6.0.min.js></script><link rel=stylesheet type=text/css href=/css/loading.css><meta name=generator content="Hexo 7.1.1"></head><body><header class=navbar-fixed><nav id=headNav class="bg-color nav-transparent"><div id=navContainer class="nav-wrapper container"><div class=brand-logo><a href=/ class="waves-effect waves-light"><img src=/medias/logo.png class=logo-img alt=LOGO> <span class=logo-span>MUG-chen&#39;s Blog</span></a></div><a href=# data-target=mobile-nav class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href=/ class="waves-effect waves-light"><i class="fas fa-home" style=zoom:.6></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href=/tags class="waves-effect waves-light"><i class="fas fa-tags" style=zoom:.6></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href=/categories class="waves-effect waves-light"><i class="fas fa-bookmark" style=zoom:.6></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href=/archives class="waves-effect waves-light"><i class="fas fa-archive" style=zoom:.6></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href=/about class="waves-effect waves-light"><i class="fas fa-user-circle" style=zoom:.6></i> <span>关于</span></a></li><li><a href=#searchModal class="modal-trigger waves-effect waves-light"><i id=searchIcon class="fas fa-search" title=搜索 style=zoom:.85></i></a></li><li><a href=javascript:; class="waves-effect waves-light" onclick=switchNightMode() title=深色/浅色模式><i id=sum-moon-icon class="fas fa-sun" style=zoom:.85></i></a></li></ul><div id=mobile-nav class="side-nav sidenav"><div class="mobile-head bg-color"><img src=/medias/logo.png class="logo-img circle responsive-img"><div class=logo-name>MUG-chen&#39;s Blog</div><div class=logo-desc>Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class=m-nav-item><a href=/ class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class=m-nav-item><a href=/tags class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class=m-nav-item><a href=/categories class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class=m-nav-item><a href=/archives class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class=m-nav-item><a href=/about class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li><div class=divider></div></li><li><a href=https://github.com/MUG-chen/MUG-chen.github.io class="waves-effect waves-light" target=_blank><i class="fab fa-github-square fa-fw"></i>Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href=https://github.com/MUG-chen/MUG-chen.github.io class="github-corner tooltipped hide-on-med-and-down" target=_blank data-tooltip="Fork Me" data-position=left data-delay=50><svg viewBox="0 0 250 250" aria-hidden=true><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill=currentColor style="transform-origin:130px 106px" class=octo-arm></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill=currentColor class=octo-body></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style=background-image:url(/medias/featureimages/11.jpg)><div class=container style=right:0;left:0><div class=row><div class="col s12 m12 l12"><div class=brand><h1 class="description center-align post-title">生成式人工智能导论-Chap.4</h1></div></div></div></div></div><main class="post-container content"><div class=row><div id=main-content class="col s12 m12 l9"><div id=artDetail><div class=card><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class=article-tag><a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ ><span class="chip bg-color">人工智能</span> </a><a href=/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/ ><span class="chip bg-color">生成式人工智能导论</span> </a><a href=/tags/Introduction-to-Generative-Artificial-Intelligence/ ><span class="chip bg-color">Introduction to Generative Artificial Intelligence</span></a></div></div><div class="col s5 right-align"><div class=post-cate><i class="fas fa-bookmark fa-fw icon-category"></i> <a href=/categories/Study-Notes/ class=post-category>Study Notes </a><a href=/categories/Study-Notes/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ class=post-category>人工智能</a></div></div></div><div class=post-info><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2026-01-09</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2026-01-09</div><div class=info-break-policy><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 1.8k</div></div></div><hr class=clearfix><link rel=stylesheet href=/libs/prism/prism.min.css><div class="card-content article-card-content"><div id=articleContent><blockquote><p>本系列生成式人工智能导论博文基于国立台湾大学李宏毅老师发布的课程视频整理而来, 仅用作复习参考.<br>可以算是人工智能领域的入门课程.<br>需要明确的是, 李宏毅老师上传的课程已经是2024年的课程, 距今有两年的时间, 知识难免有过时的可能, 如果出现了这种情况, 博主会尽可能补足当前的状态.</p></blockquote><h1 id=生成式人工智能导论-Chap-4-语言模型的内里细节与评估方式><a href=#生成式人工智能导论-Chap-4-语言模型的内里细节与评估方式 class=headerlink title="生成式人工智能导论 Chap.4 语言模型的内里细节与评估方式"></a>生成式人工智能导论 Chap.4 语言模型的内里细节与评估方式</h1><p>我们回顾下前几章中所说的:</p><ul><li>模型的本质是一个含有大量参数的函式, 其训练过程就是将这些参数确定的过程.</li><li>函式的参数量过大, 需要使用一个特定的表示方式, 在当前, 通常使用 <strong>类神经网络</strong> 来表示它.</li></ul><p>最出名的类神经网络我们也提到过, 相信对各位也耳熟能详: Transformer</p><h2 id=4-1-Transformer-概述><a href=#4-1-Transformer-概述 class=headerlink title="4.1 Transformer 概述"></a>4.1 Transformer 概述</h2><blockquote><p>模型的演进并非一步到位, 事实上, Transformer之前有很多别的模型架构, 例如RNN, Feed-forward Network等等. 不过限于本课程的导论性质, 我们只对当下最流行的Transformer进行简要介绍.</p></blockquote><p>Transformer通过以下五层, 将输入转化为概率分布进行输出:</p><ol><li>Tokenization</li><li>Input Layer</li><li>Attention</li><li>Feed Forward</li><li>Output Layer</li></ol><p>值得注意的是, 3与4这两层被并称为一个 <strong>Transformer Block</strong> , 而这一个结构事实上在一个类神经网络中具有多层, 模型会反复思考, 最终才能得到输出.</p><h3 id=4-1-1-Tokenization><a href=#4-1-1-Tokenization class=headerlink title="4.1.1 Tokenization"></a>4.1.1 Tokenization</h3><p>Tokenization这一层的目的是将输入的语句转化为Token Sequence. 简单来讲, 就是把我们的输入切成一段一段的, 这被切开的每一份叫做一个Token.</p><p>显然, 分割这个工作是需要相应的标准的 <del>(就连split函数都需要传入切割参数呢, 你说是吧)</del> , Tokenization也不例外, 它通过一个预先设定好的 <strong>Token list</strong> 来进行分割.</p><p>当然, 这个Token list怎么来呢? 方法比较五花八门. 比较常用的一个是通过一种叫 <strong>Byte Pair Encoding(BPE)</strong> 的方式, 通过大量文字以及前后文的关联连接性, 来得到Token list.</p><p><img src=https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Loading.svg data-original=https://major-course-1310677143.cos.ap-guangzhou.myqcloud.com/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/Chap.4/Tokenization.png alt=Tokenization></p><h3 id=4-1-2-Input-Layer><a href=#4-1-2-Input-Layer class=headerlink title="4.1.2 Input Layer"></a>4.1.2 Input Layer</h3><p>Input Layer的目的是让模型 <strong>理解</strong> 每一个Token都是什么意思.<br>欸, 怎么理解呢? 在机器学习中, 这个步骤在做的就是将每一个Token都变成一个Vector(向量), 这个过程也被我们称为 <strong>Embedding</strong> .</p><blockquote><p>注: 从严谨性上来看, 我们应该使用Vector, 但实际上, 我们常常直接用Embedding来直接指代某个Token的对应向量, 只是一种习惯性表述.</p></blockquote><p>为啥转化成向量, 模型就能理解词义了呢? 事实上, 原先一个Token的形式, 对机器而言只是一个标识, 但Embedding不同, 它能够被量化了, 这意味着机器可以通过计算向量间的距离来具体得知两个词的含义是否相近. 也相应地就是一种词义的理解工作.</p><p><img src=https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Loading.svg data-original=https://major-course-1310677143.cos.ap-guangzhou.myqcloud.com/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/Chap.4/Embedding.png alt=Embedding></p><p>好, 现在的问题是, 每个Token对应的Embedding是从哪里来的呢?<br>相信各位应该很容易想到, 我们只要把Token list中所有的Token的Embedding提前算出来不就好了?<br>没错! 这个东西确实存在, 并且就叫做 <strong>Token Embedding</strong> , 它本质上就是一个映射表, 将每个Token映射到其对应的Embedding上. 至于这个东西怎么来, 就是我们训练时期的工作了.</p><blockquote><p>实际上, 每个Token对应的Embedding正是组成我们模型中未知参数的一部分, 甚至可以说是一大部分.</p></blockquote><hr><p>到现在为止, 我们已经得到了一个句子中每个Token的Embedding, 但实际上, 还有个对于词义比较关键的东西, 我们没考虑:<br>这个Token在句子中的 <strong>位置</strong> .</p><p>举个简单的例子:<br>“我爱你” 跟 “你爱我” . 如果不考虑这种位置信息, 这两句话在模型中就是同一个意思. <del>(我靠, 这简直乱套了)</del></p><p>因此, 除了Token本身的Embedding之外, 还有一种Embedding需要加在我们的Input Layer中, 它被称为 <strong>Positional Embedding</strong> . 这种Positional Embedding可以人为预先定义, 当然, 也可以通过在训练过程中找出.</p><p><img src=https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Loading.svg data-original=https://major-course-1310677143.cos.ap-guangzhou.myqcloud.com/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/Chap.4/Positional%20Embedding.png alt="Positional Embedding"></p><h3 id=4-1-3-Transformer-Block><a href=#4-1-3-Transformer-Block class=headerlink title="4.1.3 Transformer Block"></a>4.1.3 Transformer Block</h3><p>Transformer Block的目的是让模型能够考虑上下文. 它接受来自上一层Input Layer的Embedding, 结合上下文信息将其转化为融合了上下文信息的Embedding, 这种Embedding也有个专属名词: <strong>Contextualized Token Embedding</strong> .</p><p>好, 现在来看看Transformer Block这两层里面究竟干了点啥.</p><hr><p><strong>Step 1</strong> Attention Weight</p><p>这一步是将我们所要结合上下文的Token(Selected Token)与这个句子中的每一个其它Token都进行一步计算, 得到两个Token之间的相关性, 这个相关性也有个专有名词: <strong>Attention Weight</strong> .<br>这个Attention Weight的计算也是通过一个函数(小模型)来进行的, 这个小模型的参数也是我们训练得到的.</p><p><img src=https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Loading.svg data-original=https://major-course-1310677143.cos.ap-guangzhou.myqcloud.com/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/Chap.4/Attention.%E8%AE%A1%E7%AE%97%E7%9B%B8%E5%85%B3%E6%80%A7.png alt="Attention.Attention Weight"></p><hr><p><strong>Step 2</strong> Weighted Sum</p><p>随后我们将计算出的相关性做一个带权加和, 计算出我们Attention层的最终输出:</p><p><img src=https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Loading.svg data-original=https://major-course-1310677143.cos.ap-guangzhou.myqcloud.com/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/Chap.4/Attention.Weighted%20Sum.png alt="Attention.Weighted Sum"></p><hr><p><strong>Attention Matrix</strong></p><p>我们会发现Attention这一层需要所有的Token两两计算出它们的Attention Weight, 这个计算过程可以跟线性代数中的矩阵一块来看:</p><p><img src=https://mug-chensblog-1310677143.cos.ap-beijing.myqcloud.com/Loading.svg data-original=https://major-course-1310677143.cos.ap-guangzhou.myqcloud.com/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/Chap.4/Attention.Attention%20Matrix.png alt="Attention.Attention Matrix"></p><hr><p><strong>更多…</strong></p><p>事实上, 在实际运作时, 我们不会考虑下文, 也就是一个Token与其后面Token的相关性, 这一机制叫 <strong>Causal Attention</strong> .</p><p>此外, 关于相关性的计算, 往往也不会只通过一个小模型来计算. 这很好理解, 不同的词汇以不同的标准来看相关性显然是不同的. 事实上, 这种计算相关性的组数往往有8组甚至16组之多. 这个机制叫 <strong>Multi-head Attention</strong> .</p><p>显然, Multi-head Attention有了多组输出, 我们后面必须再加一个模块来将多组输出整合, 也就是Transformer Block中另一层Feed Forward要干的事情. Feed Forward本身也是一个模型, 其参数通过训练的过程确定下来, 会将多组来自Attention层的输出组合, 最终形成一个整合起来的Embedding.</p><p>而往往一个模型中会有很多很多这样的Transformer Block, 当然, 学术中为了简化称呼, 常常直接将一个Transformer Block成为一个Layer, 在经过很多层Layer的演算后, 最终的结果会被传给最下方一层, Output Layer.</p><h3 id=4-1-4-Output-Layer><a href=#4-1-4-Output-Layer class=headerlink title="4.1.4 Output Layer"></a>4.1.4 Output Layer</h3><p>Output Layer的目的是拿到最后一个Layer的输出, 并将其通过Linear Transform与Softmax两步, 形成一个关于词表的概率分布.</p><p>其中, Linear Transform负责将模型提取的抽象特征向量, 映射为词表中每一个词对应的原始分值, 又叫 <strong>Logits</strong> .<br>Softmax只负责将原始分值转化为总和为 1 的概率分布, 从而明确每个词作为下一个输出的具体可能性.</p><p>至此, 我们对Transformer有了一个简要的了解.</p></div><hr><div class=reprint id=reprint-statement><div class=reprint__author><span class=reprint-meta style=font-weight:700><i class="fas fa-user">文章作者: </i></span><span class=reprint-info><a href=/about rel="external nofollow noreferrer">MUG-chen</a></span></div><div class=reprint__type><span class=reprint-meta style=font-weight:700><i class="fas fa-link">文章链接: </i></span><span class=reprint-info><a href=http://mug-chen.github.io/posts/11553.html>http://mug-chen.github.io/posts/11553.html</a></span></div><div class=reprint__notice><span class=reprint-meta style=font-weight:700><i class="fas fa-copyright">版权声明: </i></span><span class=reprint-info>本博客所有文章除特別声明外，均采用 <a href=https://creativecommons.org/licenses/by/4.0/deed.zh rel="external nofollow noreferrer" target=_blank>CC BY 4.0</a> 许可协议。转载请注明来源 <a href=/about target=_blank>MUG-chen</a> !</span></div></div><script async defer=defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class=tag_share style=display:block><div class=post-meta__tag-list style=display:inline-block><div class=article-tag><a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ ><span class="chip bg-color">人工智能</span> </a><a href=/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/ ><span class="chip bg-color">生成式人工智能导论</span> </a><a href=/tags/Introduction-to-Generative-Artificial-Intelligence/ ><span class="chip bg-color">Introduction to Generative Artificial Intelligence</span></a></div></div><div class=post_share style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel=stylesheet type=text/css href=/libs/share/css/share.min.css><div id=article-share><div class=social-share data-sites=twitter,facebook,google,qq,qzone,wechat data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src=/libs/share/js/social-share.min.js></script></div></div></div></div></div><article id=prenext-posts class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos=fade-up data-aos=fade-up><div class="article-badge left-badge text-color"><i class="far fa-dot-circle"></i>&nbsp;本篇</div><div class=card><a href=/posts/11553.html><div class=card-image><img src=/medias/featureimages/11.jpg class=responsive-img alt=生成式人工智能导论-Chap.4> <span class=card-title>生成式人工智能导论-Chap.4</span></div></a><div class="card-content article-content"><div class="summary block-with-text">生成式人工智能导论第四部分</div><div class=publish-info><span class=publish-date><i class="far fa-clock fa-fw icon-date"></i>2026-01-09 </span><span class=publish-author><i class="fas fa-bookmark fa-fw icon-category"></i> <a href=/categories/Study-Notes/ class=post-category>Study Notes </a><a href=/categories/Study-Notes/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ class=post-category>人工智能</a></span></div></div><div class="card-action article-tags"><a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ ><span class="chip bg-color">人工智能</span> </a><a href=/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/ ><span class="chip bg-color">生成式人工智能导论</span> </a><a href=/tags/Introduction-to-Generative-Artificial-Intelligence/ ><span class="chip bg-color">Introduction to Generative Artificial Intelligence</span></a></div></div></div><div class="article col s12 m6" data-aos=fade-up><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class=card><a href=/posts/13302.html><div class=card-image><img src=/medias/featureimages/10.jpg class=responsive-img alt=生成式人工智能导论-杂项> <span class=card-title>生成式人工智能导论-杂项</span></div></a><div class="card-content article-content"><div class="summary block-with-text">生成式人工智能导论 杂项部分</div><div class=publish-info><span class=publish-date><i class="far fa-clock fa-fw icon-date"></i>2026-01-06 </span><span class=publish-author><i class="fas fa-bookmark fa-fw icon-category"></i> <a href=/categories/Study-Notes/ class=post-category>Study Notes </a><a href=/categories/Study-Notes/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ class=post-category>人工智能</a></span></div></div><div class="card-action article-tags"><a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ ><span class="chip bg-color">人工智能</span> </a><a href=/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/ ><span class="chip bg-color">生成式人工智能导论</span> </a><a href=/tags/Introduction-to-Generative-Artificial-Intelligence/ ><span class="chip bg-color">Introduction to Generative Artificial Intelligence</span></a></div></div></div></div></article></div><script type=text/javascript src=/libs/codeBlock/codeBlockFuction.js></script><script type=text/javascript src=/libs/prism/prism.min.js></script><script type=text/javascript src=/libs/codeBlock/codeLang.js></script><script type=text/javascript src=/libs/codeBlock/codeCopy.js></script><script type=text/javascript src=/libs/codeBlock/codeShrink.js></script></div><div id=toc-aside class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style=background-color:#fff><div class=toc-title><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id=toc-content></div></div></div></div><div id=floating-toc-btn class=hide-on-med-and-down><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src=/libs/tocbot/tocbot.min.js></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")});const n="expanded";let i=$("#toc-aside"),l=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){i.hasClass(n)?(i.removeClass(n).hide(),l.removeClass("l9")):(i.addClass(n).show(),l.addClass("l9"));var e="artDetail",o="prenext-posts";if(0!==(e=$("#"+e)).length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#"+o).width(t)}})})</script></main><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style=margin-bottom:15px!important><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id=year>2022-2026</span> <a href=/about target=_blank>MUG-chen</a> |&nbsp;Powered by&nbsp;<a href=https://hexo.io/ target=_blank>Hexo</a> |&nbsp;Theme&nbsp;<a href=https://github.com/blinkfox/hexo-theme-matery target=_blank>Matery</a><br><span id=busuanzi_container_site_pv>&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp; <span id=busuanzi_value_site_pv class=white-color></span> </span><span id=busuanzi_container_site_uv>&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp; <span id=busuanzi_value_site_uv class=white-color></span></span><br><span id=sitetime>Loading ...</span><script>var calcSiteTime=function(){var e=new Date,t="2022",n=e.getFullYear(),i=e.getMonth()+1,a=e.getDate(),r=e.getHours(),o=e.getMinutes(),e=e.getSeconds(),s=Date.UTC(t,"12","25","22","45","0"),i=Date.UTC(n,i,a,r,o,e)-s,a=Math.floor(i/31536e6),r=Math.floor(i/864e5-365*a);t===String(n)?(document.getElementById("year").innerHTML=n,o="This site has been running for "+r+" days",o="本站已运行 "+r+" 天",document.getElementById("sitetime").innerHTML=o):(document.getElementById("year").innerHTML=t+" - "+n,e="This site has been running for "+a+" years and "+r+" days",e="本站已运行 "+a+" 年 "+r+" 天",document.getElementById("sitetime").innerHTML=e)};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href=https://github.com/MUG-chen class=tooltipped target=_blank data-tooltip=访问我的GitHub data-position=top data-delay=50><i class="fab fa-github"></i></a></div></div></footer><div class=progress-bar></div><div id=searchModal class=modal><div class=modal-content><div class=search-header><span class=title><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type=search id=searchInput name=s placeholder=请输入搜索的关键字 class=search-input></div><div id=searchResult></div></div></div><script type=text/javascript>$(function(){!function(t,r,s){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var e=$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),t=document.getElementById(r),n=document.getElementById(s);t.addEventListener("input",function(){var o='<ul class="search-result-list">',h=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length<=0||(e.forEach(function(t){var n,e,r,s=!0,i=t.title.trim().toLowerCase(),l=t.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),a=0===(a=t.url).indexOf("/")?t.url:"/"+a,c=-1,u=-1;""!==i&&""!==l&&h.forEach(function(t,e){n=i.indexOf(t),c=l.indexOf(t),n<0&&c<0?s=!1:(c<0&&(c=0),0===e&&(u=c))}),s&&(o+="<li><a href='"+a+"' class='search-result-title'>"+i+"</a>",a=t.content.trim().replace(/<[^>]+>/g,""),0<=u&&(t=u+80,(t=0===(e=(e=u-20)<0?0:e)?100:t)>a.length&&(t=a.length),r=a.substr(e,t),h.forEach(function(t){var e=new RegExp(t,"gi");r=r.replace(e,'<em class="search-keyword">'+t+"</em>")}),o+='<p class="search-result">'+r+"...</p>"),o+="</li>")}),o+="</ul>",n.innerHTML=o)})}})}("/search.xml","searchInput","searchResult")})</script><div class=stars-con><div id=stars></div><div id=stars2></div><div id=stars3></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout(function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout(function(){$(".Cuteen_DarkSky").fadeOut(1e3,function(){$(this).remove()})},2e3)})}</script><div id=backTop class=top-scroll><a class="btn-floating btn-large waves-effect waves-light" href=#!><i class="fas fa-arrow-up"></i></a></div><script src=/libs/materialize/materialize.min.js></script><script src=/libs/masonry/masonry.pkgd.min.js></script><script src=/libs/aos/aos.js></script><script src=/libs/scrollprogress/scrollProgress.min.js></script><script src=/libs/lightGallery/js/lightgallery-all.min.js></script><script src=/js/matery.js></script><script type=text/javascript>var windowWidth=$(window).width();768<windowWidth&&document.write('<script type="text/javascript" src="/libs/others/snow.js"><\/script>')</script><script src=https://ssl.captcha.qq.com/TCaptcha.js></script><script src=/libs/others/TencentCaptcha.js></script><button id=TencentCaptcha data-appid=xxxxxxxxxx data-cbfn=callback type=button hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0],e=(t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}()</script><script async src=/libs/others/busuanzi.pure.mini.js></script><script src=/libs/instantpage/instantpage.js type=module></script><div id=loading-box><div class=loading-left-bg></div><div class=loading-right-bg></div><div class=spinner-box><div class=configure-border-1><div class=configure-core></div></div><div class=configure-border-2><div class=configure-core></div></div><div class=loading-word>加载中...</div></div></div><script>window.addEventListener("load",function(){document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},!1)</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!0,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a=c[o],i=function(){c=c.filter(function(t){return a!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(a)};(t=a).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),i()):(e=new Image,n=t.getAttribute("data-original"),e.onload=function(){t.src=n,t.removeAttribute("data-original"),i()},t.src!==n&&(e.src=n))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this)</script></body></html>